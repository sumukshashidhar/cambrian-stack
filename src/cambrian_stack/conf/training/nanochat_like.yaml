# Training hyperparameters inspired by nanochat base_train defaults
device_batch_size: 32
total_batch_size: 524288
max_steps: 20000  # adjust as needed; nanochat uses target ratios/flops
learning_rate: 0.001
weight_decay: 0.0
grad_clip: 1.0
warmup_ratio: 0.0
warmdown_ratio: 0.2
final_lr_frac: 0.0
eval_every: 250
eval_batches: 20
sample_every: 2000
save_every: 0   # disabled by default; set to >0 to enable periodic saves
log_every: 10
