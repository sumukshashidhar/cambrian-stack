# Nanochat-like training hyperparameters for depth-32 runs (8xH100 baseline)
device_batch_size: 8
total_batch_size: 524288
max_steps: 71680        # aligns with tokens:param ~20 for ~1.9B params
learning_rate: 0.001
weight_decay: 0.0
grad_clip: 1.0
warmup_ratio: 0.0
warmdown_ratio: 0.2
final_lr_frac: 0.0
eval_every: 250
eval_batches: 20
sample_every: 2000
save_every: 0            # disable periodic saves by default
log_every: 10
