# Baseline Transformer Configuration
# Based on Karpathy's nanochat speedrun defaults
#
# To create a new experiment:
# 1. Copy this file to configs/my_experiment.yaml
# 2. Modify the model.type and other params
# 3. Run: ./scripts/train.sh --config-name=my_experiment

defaults:
  - _self_

model:
  type: transformer  # Options: transformer, diffusion_transformer, energy_based_transformer
  depth: 12          # d_model = depth * 64 = 768
  max_seq_len: 1024
  vocab_size: 50257  # GPT-2 tokenizer
  dropout: 0.0

data:
  dataset_name: roneneldan/TinyStories
  train_split: train
  val_split: validation
  tokenizer_name: gpt2
  num_workers: 4

training:
  # Batch size
  device_batch_size: 24       # Per-GPU, reduce if OOM
  total_batch_size: 262144    # Total tokens per optimizer step
  
  # Training horizon
  max_steps: 10000
  
  # Optimizer (AdamW)
  learning_rate: 6.0e-4
  weight_decay: 0.1
  grad_clip: 1.0
  
  # LR Schedule (Karpathy style: no warmup, cosine decay at end)
  warmup_ratio: 0.0
  warmdown_ratio: 0.2
  final_lr_frac: 0.0
  
  # Evaluation
  eval_every: 250
  eval_batches: 20
  sample_every: 1000
  save_every: 2500

logging:
  wandb_project: ${oc.env:WANDB_PROJECT,cambrian-stack}
  wandb_run: baseline-d12     # Set to null to disable wandb
  log_every: 10

output:
  dir: out/baseline-d12

