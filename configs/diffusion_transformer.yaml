# Diffusion Transformer configuration

defaults:
  - _self_

model:
  type: diffusion_transformer
  depth: 12
  max_seq_len: 256
  vocab_size: 50257
  diffusion_steps: 128
  mask_token_id: 50256
  dropout: 0.0

data:
  dataset_name: roneneldan/TinyStories
  train_split: train
  val_split: validation
  tokenizer_name: gpt2
  num_workers: 4
  corruption_rate: 0.15

training:
  device_batch_size: 24
  total_batch_size: 262144
  max_steps: 10000
  learning_rate: 3.0e-4
  weight_decay: 0.01
  grad_clip: 1.0
  warmup_ratio: 0.01
  warmdown_ratio: 0.2
  final_lr_frac: 0.1
  eval_every: 250
  eval_batches: 20
  sample_every: 1000
  save_every: 2500

logging:
  wandb_project: ${oc.env:WANDB_PROJECT,cambrian-stack}
  wandb_run: diffusion-d12
  log_every: 10

output:
  dir: out/diffusion-d12
